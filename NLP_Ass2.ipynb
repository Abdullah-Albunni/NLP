{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import library"
      ],
      "metadata": {
        "id": "o18HuqXbdEOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import warnings\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import pipeline\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"The secret `HF_TOKEN` does not exist in your Colab secrets.\")\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgTl-98NdElm",
        "outputId": "74e49b71-2449-4b37-f86c-231c8b9af006"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the text generation pipeline with the desired model\n",
        "generator = pipeline('text-generation', model='gpt2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b3YLgywpHYo",
        "outputId": "9569a858-d25d-426b-9491-b169091c1949"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocesing Data"
      ],
      "metadata": {
        "id": "UO4viqEEdEzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Remove non-alphabetic characters\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "\n",
        "    # Normalize text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize text\n",
        "    words = text.split()\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    # Remove single characters\n",
        "    words = [word for word in words if len(word) > 1]\n",
        "\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
        "\n",
        "    unique_words = set(words)\n",
        "    return ' '.join(unique_words)"
      ],
      "metadata": {
        "id": "vTWxddGLdE9Y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF using sklearn"
      ],
      "metadata": {
        "id": "GqpnzSA5dFsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_tfidf_sklearn(documents):\n",
        "    # Initialize TF-IDF vectorizer\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "    # Compute TF-IDF matrix\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "    # Get normalized TF-IDF matrix\n",
        "    tfidf_normalized_matrix = tfidf_matrix.toarray() / np.linalg.norm(tfidf_matrix.toarray(), axis=1, ord=2, keepdims=True)\n",
        "\n",
        "    return tfidf_normalized_matrix"
      ],
      "metadata": {
        "id": "CxPvzqwGdF1r"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF from scratch"
      ],
      "metadata": {
        "id": "Uk7KCeyAdF-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_tfidf_from_scratch(documents):\n",
        "    # Preprocess the documents and get unique words\n",
        "    processed_documents = [clean_text(doc).split() for doc in documents]\n",
        "    unique_words = set(word for doc in processed_documents for word in doc)\n",
        "\n",
        "    # Calculate TF for each word in all documents\n",
        "    tf_matrix_scratch = np.zeros((len(documents), len(unique_words)))\n",
        "\n",
        "    for i, doc in enumerate(processed_documents):\n",
        "        word_counts = {word: doc.count(word) for word in doc}\n",
        "        for j, word in enumerate(unique_words):\n",
        "            tf_matrix_scratch[i, j] = np.log(1 + word_counts.get(word, 0))\n",
        "\n",
        "    # Calculate IDF for each word\n",
        "    idf_matrix_scratch = np.zeros(len(unique_words))\n",
        "\n",
        "    for i, word in enumerate(unique_words):\n",
        "        num_docs_containing_word = sum(1 for doc in processed_documents if word in doc)\n",
        "        idf_matrix_scratch[i] = np.log(len(documents) / (1 + num_docs_containing_word))\n",
        "\n",
        "    # Calculate TF-IDF for each word\n",
        "    tfidf_matrix_scratch = tf_matrix_scratch * idf_matrix_scratch\n",
        "\n",
        "    # Normalize TF-IDF for each document\n",
        "    normalized_tfidf_matrix_scratch = tfidf_matrix_scratch / np.linalg.norm(tfidf_matrix_scratch, axis=1, keepdims=True)\n",
        "\n",
        "    return normalized_tfidf_matrix_scratch"
      ],
      "metadata": {
        "id": "1pU725COdFQn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate and clean Text"
      ],
      "metadata": {
        "id": "SIc5_5nMdFG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_clean_text(prompt):\n",
        "    generated_text = generator(prompt, max_length=500,  num_return_sequences=1)[0]['generated_text']\n",
        "    cleaned_text = clean_text(generated_text)\n",
        "    return cleaned_text"
      ],
      "metadata": {
        "id": "8CY4LqyQpfig"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unique words for each documnet"
      ],
      "metadata": {
        "id": "VLgiHrpPdFZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate and clean text for each prompt\n",
        "prompt1 = \"Machine learning\"\n",
        "prompt2 = \"Football is a good sport\"\n",
        "prompt3 = \"Medications are used....\"\n",
        "\n",
        "cleaned_text1 = generate_and_clean_text(prompt1)\n",
        "cleaned_text2 = generate_and_clean_text(prompt2)\n",
        "cleaned_text3 = generate_and_clean_text(prompt3)\n",
        "\n",
        "# print the unique words for each document and number of unique words in each document\n",
        "print(\"Unique Words:\")\n",
        "print(f\"Document 1: {cleaned_text1}\")\n",
        "print(f\"Number of Unique Words in Document 1: {len(cleaned_text1.split())}\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Document 2: {cleaned_text2}\")\n",
        "print(f\"Number of Unique Words in Document 2: {len(cleaned_text2.split())}\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Document 3: {cleaned_text3}\")\n",
        "print(f\"Number of Unique Words in Document 3: {len(cleaned_text3.split())}\")\n",
        "\n",
        "# Create a list of documents\n",
        "documents = [cleaned_text1, cleaned_text2, cleaned_text3]\n",
        "\n",
        "# Compute TF-IDF matrix using sklearn\n",
        "tfidf_matrix_sklearn = compute_tfidf_sklearn(documents)\n",
        "\n",
        "# Compute TF-IDF matrix from scratch\n",
        "tfidf_matrix_scratch = compute_tfidf_from_scratch(documents)\n",
        "\n",
        "# Compute cosine similarity using sklearn TF-IDF matrix\n",
        "cosine_similarity_sklearn = cosine_similarity(tfidf_matrix_sklearn)\n",
        "\n",
        "# Compute cosine similarity using TF-IDF matrix from scratch\n",
        "cosine_similarity_scratch = cosine_similarity(tfidf_matrix_scratch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWpdLhsMdFjZ",
        "outputId": "26a426af-7dac-450e-ac5a-ff20a64a572f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Words:\n",
            "Document 1: declaration going ha good set course normal compiler structure build would taken learning join really create realize machine extend feature written say everything simple signature give system definition use point per look century around make couple finally first programmer data started diagram new framework object member method regular difference result way process fact number doe convert important pretty seen immutable people thing string language library building different well python basic creating help dont clean two readable note small get let take class perfectly define expression argument provided created want know background base added return next example show top like converted called powerful concise similar im current type quite ccli objectivec program variable evaluate mentioned work code made function also probably may easy\n",
            "Number of Unique Words in Document 1: 122\n",
            "--------------------------------------------------\n",
            "Document 2: fan going always club glad tough find right evaluating good expected ranked saying er win build would really great josh guy young sport qb landscape assignment fourth san told improve jose season nfl point pitino look coker receiver see change first role drafted patriot monday head scott better coach losing solid coming new lot thats fun plenty job wide colorado important number bring youre people position thing likely west open building developing go coordinator start well team quinn two assume bunch scoring grow note school trestman get former class offensive alltime end saturday week wa run prospect year pac want winning curve game colin recruiting able playing much like many said brolin begin higher football im boy current rick offense coaching program quarterback experience staff available yeah state nflcom play probably significant one time big think\n",
            "Number of Unique Words in Document 2: 136\n",
            "--------------------------------------------------\n",
            "Document 3: detail ship mail policy ha phone doctor handled must additional receive regarding send doesnt would payment shipment delay issue deliver arrival minimum packing within customer purchasing usa purchase call give tracking often international make assured order uspending following included usedif package instruction happens dealer price country ez expect difference delivered st process important standard insurance number cost shipping location please youll allowed asia basic provide dont delivery directly noncomedogenic take usps plus required week usd expense normally year drug parcel medication want unable prior cannot buyer received like alternate item day charged consulting longer sent similar using delayed information address excluding efficient contact product shipped without complete usually destination also personally time transaction\n",
            "Number of Unique Words in Document 3: 113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare outputs"
      ],
      "metadata": {
        "id": "hijWJ5FPpxio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output cosine similarity matrices\n",
        "print(\"Cosine Similarity (Sklearn):\")\n",
        "print(cosine_similarity_sklearn)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Cosine Similarity (From Scratch):\")\n",
        "print(cosine_similarity_scratch)\n",
        "\n",
        "# Error between the two cosine similarity matrices\n",
        "error = np.mean(np.abs(cosine_similarity_sklearn - cosine_similarity_scratch))\n",
        "print(\"Mean Absolute Error between Sklearn and From Scratch:\", error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttUV9KZJdGHd",
        "outputId": "adcba25c-f1b4-4e64-e26e-00d071d3566d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity (Sklearn):\n",
            "[[1.         0.11650957 0.0715767 ]\n",
            " [0.11650957 1.         0.0307181 ]\n",
            " [0.0715767  0.0307181  1.        ]]\n",
            "\n",
            "\n",
            "Cosine Similarity (From Scratch):\n",
            "[[1.         0.02530586 0.02693987]\n",
            " [0.02530586 1.         0.02424565]\n",
            " [0.02693987 0.02424565 1.        ]]\n",
            "Mean Absolute Error between Sklearn and From Scratch: 0.03162510644422607\n"
          ]
        }
      ]
    }
  ]
}